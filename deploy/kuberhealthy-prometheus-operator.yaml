---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: khchecks.comcast.github.io
spec:
  group: comcast.github.io
  version: v1
  scope: Namespaced
  names:
    plural: khchecks
    singular: khcheck
    kind: KuberhealthyCheck
    shortNames:
      - khc
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: khstates.comcast.github.io
spec:
  group: comcast.github.io
  version: v1
  scope: Namespaced
  names:
    plural: khstates
    singular: khstate
    kind: KuberhealthyState
    shortNames:
      - khs
---
# Source: kuberhealthy/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name:  kuberhealthy-pdb
  namespace: kuberhealthy
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: kuberhealthy
---
# Source: kuberhealthy/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: "prometheus"
    prometheus: prometheus
    role: alert-rules
  name: kuberhealthy
  namespace: kuberhealthy
data:
  kuberhealthy.rules: |-
    groups:
    - name: ./kuberhealthy.rules
      rules:
      - alert: KuberhealthyError
        expr: kuberhealthy_running < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          description: Kuberhealthy is not healthy
      - alert: ClusterUnhealthy
        expr: kuberhealthy_cluster_state < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          description: Kuberhealthy shows that the cluster is not healthy---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kuberhealthy
data:
  kuberhealthy.yaml: |-
    listenAddress: ":8080" # The port for kuberhealthy to listen on for web requests
    enableForceMaster: false # Set to true to enable local testing, forced master mode
    logLevel: "debug" # Log level to be used
    influxUsername: "" # Username for the InfluxDB instance
    influxPassword: "" # Password for the InfluxDB instance
    influxURL: "" # Address for the InfluxDB instance
    influxDB: "http://localhost:8086" # Name of the InfluxDB database
    enableInflux: false # Set to true to enable metric forwarding to Infux DB
---
# Source: kuberhealthy/templates/check-reaper.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: check-reaper
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: daemonset-khcheck
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: deployment-sa
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kuberhealthy
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/clusterrole.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRole
metadata:
  name: kuberhealthy-daemonset-khcheck
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
---
# Source: kuberhealthy/templates/clusterrole.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRole
metadata:
  name: kuberhealthy
rules:
  - apiGroups:
    - apps
    resources:
    - daemonsets
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - extensions
    resources:
    - daemonsets
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - comcast.github.io
    resources:
    - khstates
    - khchecks
    verbs:
    - "*"
  - apiGroups:
    - ""
    resources:
    - namespaces
    - componentstatuses
    - nodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods/eviction
    verbs:
    - create
---
# Source: kuberhealthy/templates/check-reaper.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: check-reaper
  namespace: kuberhealthy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
  - kind: ServiceAccount
    name: check-reaper
    namespace: kuberhealthy
---
# Source: kuberhealthy/templates/clusterrolebinding.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRoleBinding
metadata:
  name: kuberhealthy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kuberhealthy
subjects:
- kind: ServiceAccount
  name: kuberhealthy
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/clusterrolebinding.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRoleBinding
metadata:
  name: kuberhealthy-daemonset-khcheck
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kuberhealthy-daemonset-khcheck
subjects:
- kind: ServiceAccount
  name: daemonset-khcheck
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ds-admin
  namespace: kuberhealthy
rules:
  - apiGroups:
      - ""
      - extensions
      - apps
    resources:
      - daemonsets
      - pods
    verbs:
      - create
      - delete
      - deletecollection
      - get
      - list
      - patch
      - update
      - watch
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: deployment-service-role
  namespace: kuberhealthy
rules:
  - apiGroups:
      - "apps"
    resources:
      - deployments
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: daemonset-khcheck
  namespace: kuberhealthy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ds-admin
subjects:
  - kind: ServiceAccount
    name: daemonset-khcheck
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deployment-check-rb
  namespace: kuberhealthy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: deployment-service-role
subjects:
  - kind: ServiceAccount
    name: deployment-sa
---
# Source: kuberhealthy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kuberhealthy
  name: kuberhealthy
  namespace: kuberhealthy
  annotations:
spec:
  type: ClusterIP
  ports:
  - port: 80
    name: http
    targetPort: http
  selector:
    app: kuberhealthy
---
# Source: kuberhealthy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kuberhealthy
  namespace: kuberhealthy
  labels:
    app: kuberhealthy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kuberhealthy
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: kuberhealthy
    spec:
      volumes:
        - name: config-volume
          configMap:
            # Provide the name of the ConfigMap containing the files you want
            # to add to the container
            name: kuberhealthy
      serviceAccountName: kuberhealthy
      automountServiceAccountToken: true
      containers:
      - image: kuberhealthy/kuberhealthy:v2.2.0
        command: [/app/kuberhealthy]
        ports:
        - containerPort: 8080
          name: http
        securityContext:
          runAsNonRoot: true
          runAsUser: 999
          allowPrivilegeEscalation: false
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          initialDelaySeconds: 2
          periodSeconds: 4
          successThreshold: 1
          tcpSocket:
            port: 8080
          timeoutSeconds: 1
        name: kuberhealthy
        volumeMounts:
          - name: config-volume
            mountPath: /etc/config/
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 2
          periodSeconds: 4
          successThreshold: 1
          tcpSocket:
            port: 8080
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 400m
            memory: 300Mi
          limits:
            cpu: 2
            memory: 1Gi
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
---
# Source: kuberhealthy/templates/check-reaper.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: check-reaper
  namespace: kuberhealthy
spec:
  schedule: "*/3 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: check-reaper
              image: kuberhealthy/check-reaper:v1.4.0
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsNonRoot: true
                runAsUser: 999
                allowPrivilegeEscalation: false
          restartPolicy: OnFailure
          serviceAccountName: check-reaper
  concurrencyPolicy: Forbid
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: comcast.github.io/v1
kind: KuberhealthyCheck
metadata:
  name: daemonset
  namespace: kuberhealthy
spec:
  runInterval:  15m
  timeout: 12m
  podSpec:
    securityContext:
      runAsUser: 999
      fsGroup: 999
    containers:
      - env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CHECK_POD_TIMEOUT
            value: "10m"
        image: kuberhealthy/daemonset-check:v3.1.0
        imagePullPolicy: IfNotPresent
        name: main
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    serviceAccountName: daemonset-khcheck
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: comcast.github.io/v1
kind: KuberhealthyCheck
metadata:
  name: deployment
  namespace: kuberhealthy
spec:
  runInterval:  10m
  timeout: 15m
  podSpec:
    securityContext:
      runAsUser: 999
      fsGroup: 999
    containers:
    - name: deployment
      image: kuberhealthy/deployment-check:v1.6.0
      imagePullPolicy: IfNotPresent
      env:
        - name: CHECK_DEPLOYMENT_REPLICAS
          value: "4"
        - name: CHECK_DEPLOYMENT_ROLLING_UPDATE
          value: "true"
        - name: CHECK_SERVICE_ACCOUNT
          value: "default"
      resources:
        requests:
          cpu: 25m
          memory: 15Mi
        limits:
          cpu: 40m
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
      restartPolicy: Never
    serviceAccountName: deployment-sa
    terminationGracePeriodSeconds: 60
---
# Source: kuberhealthy/templates/khcheck-dns.yaml
apiVersion: comcast.github.io/v1
kind: KuberhealthyCheck
metadata:
  name: dns-status-internal
  namespace: kuberhealthy
spec:
  runInterval: 2m
  timeout: 15m
  podSpec:
    securityContext:
      runAsUser: 999
      fsGroup: 999
    containers:
      - env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: HOSTNAME
            value: "kubernetes.default"
        image: kuberhealthy/dns-resolution-check:v1.4.0
        imagePullPolicy: IfNotPresent
        name: main
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
---
# Source: kuberhealthy/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: kuberhealthy
    prometheus: prometheus
    release: prometheus-operator
  name: kuberhealthy
  namespace: kuberhealthy
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: kuberhealthy
  namespaceSelector:
    matchNames:
      - kuberhealthy
  endpoints:
  - port: http
    interval: 15s
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
