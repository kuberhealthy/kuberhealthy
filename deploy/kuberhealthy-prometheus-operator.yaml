---
# Source: kuberhealthy/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name:  kuberhealthy-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: kuberhealthy
      chart: kuberhealthy
---
# Source: kuberhealthy/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: "prometheus"
    prometheus: prometheus
    role: alert-rules
  name: kuberhealthy
data:
  kuberhealthy.rules: |-
    groups:
    - name: ./kuberhealthy.rules
      rules:
      - alert: KuberhealthyError
        expr: kuberhealthy_running < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          description: Kuberhealthy is not healthy
      - alert: ClusterUnhealthy
        expr: kuberhealthy_cluster_state < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          description: Kuberhealthy shows that the cluster is not healthy
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: daemonset-khcheck
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: deployment-sa
---
# Source: kuberhealthy/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kuberhealthy
---
# Source: kuberhealthy/templates/clusterrole.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRole
metadata:
  name: kuberhealthy
rules:
  - apiGroups:
    - apps
    resources:
    - daemonsets
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - extensions
    resources:
    - daemonsets
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - create
    - delete
    - deletecollection
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - comcast.github.io
    resources:
    - khstates
    - khchecks
    verbs:
    - "*"
  - apiGroups:
    - ""
    resources:
    - namespaces
    - componentstatuses
    - nodes
    verbs:
    - get
    - list
    - watch
---
# Source: kuberhealthy/templates/clusterrolebinding.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRoleBinding
metadata:
  name: kuberhealthy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kuberhealthy
subjects:
- kind: ServiceAccount
  name: kuberhealthy
  namespace: kuberhealthy
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ds-admin
rules:
  - apiGroups:
      - ""
      - extensions
      - app
    resources:
      - daemonsets
      - pods
    verbs:
      - create
      - delete
      - deletecollection
      - get
      - list
      - patch
      - update
      - watch
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: deployment-service-role
rules:
  - apiGroups:
      - "apps"
    resources:
      - deployments
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: daemonset-khcheck
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ds-admin
subjects:
  - kind: ServiceAccount
    name: daemonset-khcheck
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deployment-check-rb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: deployment-service-role
subjects:
  - kind: ServiceAccount
    name: deployment-sa
---
# Source: kuberhealthy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kuberhealthy
  name: kuberhealthy
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "80"
    prometheus.io/path: "/metrics"
spec:
  type: ClusterIP
  ports:
  - port: 80
    name: http
    targetPort: http
  selector:
    app: kuberhealthy
---
# Source: kuberhealthy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kuberhealthy
  labels:
    app: kuberhealthy
    chart: kuberhealthy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kuberhealthy
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: kuberhealthy
        chart: kuberhealthy
    spec:
      serviceAccountName: kuberhealthy
      automountServiceAccountToken: true
      containers:
      - image: quay.io/comcast/kuberhealthy:2.0.0
        command: [/app/kuberhealthy]
        ports:
        - containerPort: 8080
          name: http
        securityContext:
          runAsNonRoot: true
          runAsUser: 999
          allowPrivilegeEscalation: false
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          initialDelaySeconds: 2
          periodSeconds: 4
          successThreshold: 1
          tcpSocket:
            port: 8080
          timeoutSeconds: 1
        name: kuberhealthy
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 2
          periodSeconds: 4
          successThreshold: 1
          tcpSocket:
            port: 8080
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 400m
            memory: 300Mi
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
---
# Source: kuberhealthy/templates/khcheck-daemonset.yaml
apiVersion: comcast.github.io/v1
kind: KuberhealthyCheck
metadata:
  name: daemonset
spec:
  runInterval: 15m
  timeout: 12m
  podSpec:
    containers:
      - env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CHECK_POD_TIMEOUT
            value: "10m"
        image: quay.io/comcast/kh-daemonset-check:1.0.0
        imagePullPolicy: IfNotPresent
        name: main
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
    serviceAccountName: daemonset-khcheck
---
# Source: kuberhealthy/templates/khcheck-deployment.yaml
apiVersion: comcast.github.io/v1
kind: KuberhealthyCheck
metadata:
  name: deployment
spec:
  runInterval: 30m
  timeout: 10m
  podSpec:
    containers:
    - name: deployment
      image: quay.io/comcast/deployment-check:1.0.3
      imagePullPolicy: IfNotPresent
      env:
        - name: CHECK_DEPLOYMENT_REPLICAS
          value: "4"
        - name: CHECK_DEPLOYMENT_ROLLING_UPDATE
          value: "true"
      resources:
        requests:
          cpu: 25m
          memory: 15Mi
        limits:
          cpu: 40m
      restartPolicy: Always
    serviceAccountName: deployment-sa
    terminationGracePeriodSeconds: 60
---
# Source: kuberhealthy/templates/khcheck-dns.yaml
apiVersion: comcast.github.io/v1
kind: KuberhealthyCheck
metadata:
  name: dns-status-internal
spec:
  runInterval: 2m
  timeout: 15m
  podSpec:
    containers:
      - env:
          - name: CHECK_POD_TIMEOUT
            value: "110s"
          - name: HOSTNAME
            value: "kubernetes.default"
        image: quay.io/comcast/dns-status-check:1.0.0
        imagePullPolicy: IfNotPresent
        name: main
        resources:
          requests:
            cpu: 10m
            memory: 50Mi
---
# Source: kuberhealthy/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: kuberhealthy
    chart: kuberhealthy
    prometheus: prometheus
  name: kuberhealthy
spec:
  jobLabel: component
  selector:
    matchLabels:
      app: kuberhealthy
  namespaceSelector:
    matchNames:
      - kuberhealthy
  endpoints:
  - port: http
    interval: 15s
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
